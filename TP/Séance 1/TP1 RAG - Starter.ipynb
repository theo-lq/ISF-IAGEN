{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8514812a",
   "metadata": {},
   "source": [
    "# TP2 - Retrieval Augmented Generation\n",
    "\n",
    "Dans ce TP nous allons construire un système RAG complet : base de connaissance, vectorisation et appel avec un modèle de langage.\n",
    "\n",
    "Certaines fonctions seront réutilisées dans les prochaines séances, nous encourageons donc la définition de fonction générale, optimisée et robuste. Il est à garder en tête que ce notebook n'a qu'une portée pédagogique et n'est pas forcément à jour puisque le domaine évolue rapidement.\n",
    "\n",
    "Dans ce TP nous cherchons à apporter des connaissances Machine Learning, bien que le modèle en ait largement, en utilisant des cours au format PDF à notre disposition. \n",
    "\n",
    "\n",
    "## Constitution de la base de connaissance\n",
    "\n",
    "Pour construire un RAG, il faut commencer par une base de connaissance. Elle sera composée dans notre cas de document PDF. Nous allons commencer par extraire les informations texte contenue dans les documents.\n",
    "\n",
    "**Consigne** : À partir des fichiers disponible, construire une fonction `pdf_parser` qui prend en paramètre le nom du fichier et qui renvoie le texte associé. On utilisera la classe [`PyPDFLoader`](https://python.langchain.com/docs/how_to/document_loader_pdf/#simple-and-fast-text-extraction) et sa méthode `load` pour charger le document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4a00a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77905595",
   "metadata": {},
   "source": [
    "**Consigne** : Utiliser la fonction `pdf_parser` pour charger le fichier 'ML.pdf' puis inspecter son contenu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec332e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0473470e",
   "metadata": {},
   "source": [
    "Nous avons du texte et des métadonnées. Nous commençerons par nous concentrer sur le texte. Pour qu'il puisse être digérer par le RAG, nous devons le découper en plusieurs *chunk*. La classe [`CharacterTextSplitter`](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.CharacterTextSplitter.html) permet de réaliser cette opération."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea1f928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=256,\n",
    "    chunk_overlap=0,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_documents(documents=ml_doc)\n",
    "print(f\"Il y a {len(texts)} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d05d6a",
   "metadata": {},
   "source": [
    "**Consigne** : Après avoir inspecté le contenu de la variable *texts*, afficher la distribution de la longueur des chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30cc5de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43bf41cd",
   "metadata": {},
   "source": [
    "Nous observons des chunks avec très peu de caractères. Inspecter les contenus des documents avec moins de 100 caractères et noter les améliorations possibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d300959",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in texts:\n",
    "    if len(doc.page_content) < 100:\n",
    "        print(doc.page_content)\n",
    "        print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69b2033",
   "metadata": {},
   "source": [
    "Nous avons à présent un ensemble de chunk, il nous reste à construire l'embedding pour stocker toute ces informations. Nous faisons les choix suivants :\n",
    "* Nous utiliserons l'embedding [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) pour sa taille et son entraînement spécifique à notre tâche\n",
    "* Nous utiliserons le *vector store* [FAISS](https://python.langchain.com/docs/integrations/vectorstores/faiss/) puisque nous l'avons couvert en cours.\n",
    "* Nous récupérerons les trois chunks les plus proches, pour commencer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40021b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import os\n",
    "\n",
    "os.environ['USE_TF'] = 'false'\n",
    "os.environ['USE_TORCH'] = 'true'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectordb = FAISS.from_documents(texts, embedding_model)\n",
    "n_doc_to_retrieve = 3\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": n_doc_to_retrieve})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed148169",
   "metadata": {},
   "source": [
    "Notre base de connaissance est réalisée ! Passons maintenant à l'augmentation du modèle de langage.\n",
    "\n",
    "## Génération\n",
    "\n",
    "Pour cette étape, il nous reste à définir le modèle de langage et comment nous allons nous adresser à lui.\n",
    "\n",
    "**Consigne** : Définir la variable *model* à partir de la classe [OllamaLLM](https://python.langchain.com/api_reference/ollama/llms/langchain_ollama.llms.OllamaLLM.html#ollamallm) et du modèle de votre choix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abfbda6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d42c7f56",
   "metadata": {},
   "source": [
    "**Consigne** : À l'aide de la classe [PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html#langchain_core.prompts.prompt.PromptTemplate) et en s'inspirant éventuellement de [cet exemple](https://smith.langchain.com/hub/rlm/rag-prompt), définir un template de prompt qui aura deux *input_variable* : 'context' et 'question'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3c7729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0da52ea4",
   "metadata": {},
   "source": [
    "Pour construire la chaîne de RAG, LangChain utilise le [LangChain Expression Language (LCEL)](https://python.langchain.com/v0.2/docs/concepts/#langchain-expression-language-lcel), voici dans notre cas comment cela se traduit :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51afe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt_template\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db86940",
   "metadata": {},
   "source": [
    "Une fois la chaîne définie, nous pouvons lui poser des questions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02444b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Quelle est la citation d'Alan Turing ?\"\n",
    "result = rag_chain.invoke(query)\n",
    "print(\"Answer:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffe0531",
   "metadata": {},
   "source": [
    "LangChain ne permet pas nativement d'afficher quels chunks ont été utilisé pour produire la réponse, ni le score de similarité. Pour le faire, nous allons utiliser directement FAISS.\n",
    "\n",
    "**Consigne** : À l'aide de la méthode [`similarity_search_with_score`](https://python.langchain.com/v0.2/docs/integrations/vectorstores/llm_rails/#similarity-search-with-score) de `FAISS`, afficher les trois documents utilisé dans le RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d81fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6aeeadf8",
   "metadata": {},
   "source": [
    "Nous avons finalement bien défini notre premier RAG !\n",
    "\n",
    "## Amélioration de notre RAG\n",
    "\n",
    "Mais nous pouvons faire mieux, notamment afficher la source dans la génération pour que l'utilisateur puisse vérifier et mesurer les performances de notre RAG. Une fois que nous aurons réalisé ces deux améliorations, alors nous pourrons modifier plusieurs points techniques spécifique et mesurer l'apport en performance.\n",
    "\n",
    "### Exploiter les méta-données\n",
    "\n",
    "Nous avons utilisé la classe `PyPDFLoader` qui charge chaque page dans un document. Nous avons largement utilisé le contenu *page_content* mais l'attribut *metadata* contient deux informations qui nous intéressent : *source* et *page*. \n",
    "\n",
    "**Consigne** : Modifier la fonction `format_doc` pour qu'elle prenne en paramètre une liste de document LangChain puis qu'elle affiche la source et la page en plus de seulement le contenu texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae9a90c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0363d832",
   "metadata": {},
   "source": [
    "Maintenant que nous passons des informations sur les métadonnées, il faut s'assurer que le modèle de langage les utilises.\n",
    "\n",
    "**Consigne** : Modifier le prompt template défini plus tôt pour intégrer cette règle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57e10a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "260f39f4",
   "metadata": {},
   "source": [
    "Testons à présent avec la même question sur une nouvelle chaîne RAG prenant en compte nos améliorations.\n",
    "\n",
    "**Consigne** : Définir un nouveau RAG prenant en compte les informations des méta-données, puis poser la même question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3824802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "973dfa8d",
   "metadata": {},
   "source": [
    "C'est ce que nous souhaitions obtenir ! Mais nous pourrions avoir un format un peu plus structuré et moins libre. Pour cela, nous allons modifier notre système pour qu'il renvoie des JSON !\n",
    "Commençons par modifier le template de prompt pour lui donner les instructions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4892e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "  template=\"\"\"\n",
    "    You are an assistant for question-answering tasks, use the retrieved context to answer the question.  Each piece of context includes metadata (source + page).\n",
    "    If you don’t know the answer, respond with: {{\"answer\": \"I don't know\", \"sources\": []}}\n",
    "    Otherwise, return your answer in JSON with this exact structure:\n",
    "    {{\n",
    "      \"answer\": \"your answer here\",\n",
    "      \"sources\": [\"source:page\", \"source:page\"]\n",
    "    }}\n",
    "    Rules:\n",
    "    - Answer in the same language as the question.\n",
    "    - Always include the sources (source:page).\n",
    "    - Never add extra fields.\n",
    "\n",
    "    Question: {question}\n",
    "    Context:\\n{context}\n",
    "    Answer:\n",
    "    \"\"\",\n",
    "  input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e34935",
   "metadata": {},
   "source": [
    "Puisque nous demandons ici de répondre par exemple : '['ML.pdf:91\"], nous allons lui faciliter la tâche en modifiant la fonction `format_docs`.\n",
    "\n",
    "**Consigne** : Modifier la fonction `format_docs` pour prendre en compte le formattage 'source:page'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547f6ea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0238f9f6",
   "metadata": {},
   "source": [
    "Si nous souhaitons obtenir un JSON, ou un dictionnaire, en sortie du modèle, nous devons modifier la chaîne RAG définie précédemment.\n",
    "\n",
    "**Consigne** : Remplacer la fonction [`JsonOutputParser`](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.json.JsonOutputParser.html) à la place de [`StrOutputParser`](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html#langchain_core.output_parsers.string.StrOutputParser) puis tester la nouvelle chaîne RAG avec la même question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f90db7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3db037d1",
   "metadata": {},
   "source": [
    "C'est mieux ! Il nous reste à présent à mesurer la performance de notre système.\n",
    "\n",
    "\n",
    "### Mesurer les performances\n",
    "\n",
    "Nous avons défini manuellement plusieurs questions dont les réponses sont contenus dans le cours dans le fichier JSON *eval_dataset*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4398984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"eval_dataset.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    eval_dataset = json.load(file)\n",
    "\n",
    "print(eval_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b8eb75",
   "metadata": {},
   "source": [
    "Il sera probablement difficile de mesurer la performance de manière frontale. Ainsi, nous optons pour une méthodologie *LLM as a Judge*.\n",
    "\n",
    "**Consigne** : Définir une fonction `evaluate_rag` qui prend en paramètre une chaîne RAG et un dataset pour évaluation. La fonction renverra une liste de dictionnaire avec pour clés :\n",
    "* *question* : la question posée\n",
    "* *expected_answer* : la réponse attendue\n",
    "* *predicted_answer* : la réponse obtenue\n",
    "* *expected_sources* : la ou les sources attendues\n",
    "* *predicted_sources* : la ou les sources obtenues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3a70a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da59e623",
   "metadata": {},
   "source": [
    "**Consigne** : Tester la fonction précédente avec les trois premières questions puis afficher le résultat sous la forme d'un dataframe pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33db551",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14393690",
   "metadata": {},
   "source": [
    "Nous sommes capable d'obtenir un ensemble de réponse de la part d'un modèle avec un RAG, il nous reste à mettre en place le juge.\n",
    "\n",
    "**Consigne** : Définir un prompt pour décrire le rôle du juge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eacd88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc714900",
   "metadata": {},
   "source": [
    "**Consigne** : Définir une chaîne pour le juge, de la même manière que le RAG : prompt --> model --> JSONParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c30cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6069627d",
   "metadata": {},
   "source": [
    "**Consigne** : Modifier la fonction `evaluate_rag` pour qu'elle note directement la performance du modèle et renvoie sous forme d'un dataframe pandas les résultats. On implémentera également des mesures temporelles pour le RAG et le juge, ainsi que des blocs *try...except...* pour ne pas bloquer l'exécution de toutes les requêtes si une renvoie une erreur.\n",
    "Pour pouvoir suivre l'avancement de l'évaluation, on utilisera la barre de progression tqdm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0556cbed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73d842ea",
   "metadata": {},
   "source": [
    "**Consigne** : Utiliser cette fonction sur les trois premières question du dataset d'évaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afad101d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91231c6d",
   "metadata": {},
   "source": [
    "**Consigne** : A partir des résultats précédents, donner des statistiques de performance du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d821db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "289c97f8",
   "metadata": {},
   "source": [
    "## Pour aller plus loin\n",
    "\n",
    "Nous avons plusieurs axes d'améliorations, de manière non exhaustive :\n",
    "* Une meilleure récupération du texte dans le PDF : par exemple utiliser [Docling](https://python.langchain.com/docs/integrations/document_loaders/docling/) ?\n",
    "* Une meilleure manière de découper en *chunk* le texte : par exemple utiliser [RecursiveCharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html#recursivecharactertextsplitter), ou changer la taille des chunks...\n",
    "* Un meilleur modèle d'embedding : voir le [leaderboard](https://huggingface.co/spaces/mteb/leaderboard) des embeddings\n",
    "* Un meilleur retrieval : meilleure méthode pour chercher, par exemple [MMR](https://python.langchain.com/v0.2/docs/how_to/example_selectors_mmr/)\n",
    "* De meilleurs prompt\n",
    "* Une meilleure mesure de performance : plus de questions par exemple\n",
    "\n",
    "Nous encourageons l'étudiant à tester la ou les améliorations qu'ils souhaitent faire et surtout que les apports soit mesurés séparemment. On encourage également d'utiliser ses propres documents et son propre benchmark.\n",
    "Pour accélérer encore un peu l'évaluation, on propose une version asynchrone de la fonction d'évaluation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae5fd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "async def evaluate_rag_async(rag_chain, dataset, judge_chain, max_concurrency=5):\n",
    "    \"\"\"\n",
    "    Async evaluation of a RAG chain against a dataset using a judge LLM.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    semaphore = asyncio.Semaphore(max_concurrency)\n",
    "\n",
    "    async def process_example(example):\n",
    "        async with semaphore:\n",
    "            rag_start = time.time()\n",
    "            try:\n",
    "                prediction = await rag_chain.ainvoke(example[\"question\"])\n",
    "            except Exception as e:\n",
    "                prediction = {\"answer\": \"\", \"sources\": []}\n",
    "                print(f\"[RAG ERROR] Question: {example['question']} | {e}\")\n",
    "            rag_end = time.time()\n",
    "\n",
    "            judge_input = {\n",
    "                \"question\": example[\"question\"],\n",
    "                \"expected_answer\": example[\"answer\"],\n",
    "                \"predicted_answer\": prediction.get(\"answer\", \"\"),\n",
    "                \"expected_sources\": example[\"sources\"],\n",
    "                \"predicted_sources\": prediction.get(\"sources\", []),\n",
    "            }\n",
    "\n",
    "            judge_start = time.time()\n",
    "            try:\n",
    "                judgment = await judge_chain.ainvoke(judge_input)\n",
    "            except Exception as e:\n",
    "                judgment = {\"answer_correct\": False, \"sources_correct\": False, \"explanation\": f\"Judge error: {e}\"}\n",
    "                print(f\"[JUDGE ERROR] Question: {example['question']} | {e}\")\n",
    "            judge_end = time.time()\n",
    "\n",
    "            results.append({\n",
    "                **judge_input,\n",
    "                **judgment,\n",
    "                \"rag_time\": rag_end - rag_start,\n",
    "                \"judge_time\": judge_end - judge_start,\n",
    "                \"total_time\": judge_end - rag_start\n",
    "            })\n",
    "\n",
    "    tasks = [process_example(example) for example in dataset]\n",
    "    for f in tqdm_asyncio.as_completed(tasks, desc=\"Evaluating RAG\", total=len(dataset)):\n",
    "        await f\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
