{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "172a7a9f",
   "metadata": {},
   "source": [
    "# TP2 - Benchmark automatique\n",
    "\n",
    "Dans ce TP nous allons définir une fonction pour mesurer les performances d'un modèle de langage via l'exécution de plusieurs benchmarks. Nous avons vu en cours trois manières de mesurer la performance d'un modèle de langage qu'on peut résumer à:\n",
    "1. **Évaluation automatique**: via un ensemble de questions dont on connait la réponse\n",
    "2. **Évaluation humaine**: qualification humaine de la réponse d'un modèle à une question\n",
    "3. **Évaluation par modèle de langage**: notation ou comparaison de réponse d'un ou plusieurs modèles par un autre modèle\n",
    "\n",
    "Nous nous intéressons ici au premier point, en particulier avec les benchmarks [GSM8K](https://huggingface.co/datasets/openai/gsm8k) et [HellaSwag](https://huggingface.co/datasets/Rowan/hellaswag).\n",
    "Dans l'ensemble du notebook nous utiliserons la librairie LangChain.\n",
    "\n",
    "Il est à garder en tête que ce notebook n'a qu'une portée pédagogique et n'est pas forcément à jour puisque le domaine évolue rapidement, ni que les pratiques sont celles validées par l'industrie.\n",
    "\n",
    "## Uniformisation des benchmarks\n",
    "\n",
    "Pour chaque benchmark que l'on considère, nous avons besoin de plusieurs informations :\n",
    "* **Dataset** : une fonction pour charger les questions du benchmark\n",
    "* **Référence** : une fonction capable d'identifier la réponse attentue\n",
    "* **Prompt** : un prompt qui permet de demander correctement au modèle de répondre à la question\n",
    "* **Chaîne** : une fonction qui renvoie la chaîne de traitement de LangChain\n",
    "* **Score** : une fonction qui score la performance d'un modèle sur une question\n",
    "\n",
    "Nous allons commencer par créer une classe qui regroupe ces desiderata :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cd75374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "class Benchmark:\n",
    "    name: str\n",
    "\n",
    "    def __init__(self, prompt: PromptTemplate):\n",
    "        self.prompt = prompt\n",
    "\n",
    "    def load_data(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build_chain(self, model) -> Runnable:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_reference(self, sample):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def score(self, prediction, reference):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ab41df",
   "metadata": {},
   "source": [
    "Pour rendre cette classe plus concrète, commençons par travailler avec le benchmark [GSM8K](https://huggingface.co/datasets/openai/gsm8k).\n",
    "\n",
    "### Benchmark GSM8K\n",
    "\n",
    "On commence par charger le dataset et observer une question.\n",
    "\n",
    "**Consigne** : Résoudre la question *à la main* et vérifier votre réponse. On recommande d'explorer plusieurs questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "93979ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions: 1319\n",
      "Example of question:\n",
      " Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
      "And its answer:\n",
      " Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.\n",
      "She makes 9 * 2 = $<<9*2=18>>18 every day at the farmer’s market.\n",
      "#### 18\n"
     ]
    }
   ],
   "source": [
    "import numpy as np; np.random.seed(42)\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"gsm8k\", \"main\")\n",
    "dataset = dataset[\"test\"]\n",
    "\n",
    "print(f\"Number of questions: {len(dataset)}\")\n",
    "index = 0\n",
    "print(\"Example of question:\\n\", dataset[index][\"question\"])\n",
    "print(\"And its answer:\\n\", dataset[index][\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d797f0",
   "metadata": {},
   "source": [
    "Après avoir inspecté plusieurs éléments du dataset, on remarque que la réponse finale est placée après la chaîne de caractères \"####\".\n",
    "\n",
    "**Consigne**: Construire une fonction `get_reference` qui prend en argument un élément de GMS8K (dictionnaire avec question et réponse) et renvoie la réponse attendue (string). On pourra utiliser la fonction [`search`](https://docs.python.org/3/library/re.html#re.search) de la librairie [`re`](https://docs.python.org/3/library/re.html#).\n",
    "Puis tester cette fonction sur l'exemple précédent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b336056a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: 18\n"
     ]
    }
   ],
   "source": [
    "from re import search\n",
    "\n",
    "def get_reference(sample: dict) -> str:\n",
    "    match = search(r\"#### (\\d+)\", sample[\"answer\"])\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "index = 0\n",
    "reference = get_reference(sample=dataset[index])\n",
    "print(f\"Reference: {reference}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c137e6a",
   "metadata": {},
   "source": [
    "Il nous reste maintenant à définir un prompt tel que l'on puisse appeler un modèle et tester notre mécanique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b899872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=(\n",
    "        \"\"\"You are a careful mathematician. Solve the problem step by step, then display your answer in the end.\n",
    "        Question: {question}\n",
    "        Answer:\"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36433b53",
   "metadata": {},
   "source": [
    "En intégrant l'appel à un modèle via Ollama sur notre ordinateur, on peut définir avec LangChain :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f0676b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model answer : Here's how we can solve this problem step by step:\n",
      "\n",
      "1. **Calculate the total number of eggs laid:** Janet's ducks lay 16 eggs per day.\n",
      "\n",
      "2. **Calculate the number of eggs eaten:** She eats 3 eggs per day.\n",
      "\n",
      "3. **Calculate the number of eggs remaining after breakfast:** 16 eggs (laid) - 3 eggs (eaten) = 13 eggs\n",
      "\n",
      "4. **Calculate the number of eggs used for baking:** She uses 4 eggs for baking.\n",
      "\n",
      "5. **Calculate the number of eggs remaining after baking:** 13 eggs - 4 eggs (baking) = 9 eggs\n",
      "\n",
      "6. **Calculate the earnings from selling the remaining eggs:** She sells 9 eggs at $2 per egg.  So she makes 9 * $2 = $18.\n",
      "\n",
      "**Answer:** $18\n",
      "The answer was : 18\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "model = OllamaLLM(model=\"gemma3:4b\")\n",
    "\n",
    "chain = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "index = 0\n",
    "\n",
    "question = dataset[index][\"question\"]\n",
    "answer = get_reference(dataset[index])\n",
    "response = chain.invoke(question)\n",
    "print(f\"Model answer : {response}\")\n",
    "print(f\"The answer was : {answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dd7db7",
   "metadata": {},
   "source": [
    "Il nous faut extraire la dernière valeur numérique pour obtenir automatiquement la réponse du modèle.\n",
    "\n",
    "**Consigne** : Définir une fonction `score` qui prend en paramètre la réponse du modèle et la réponse attendue puis renvoie si les deux réponses sont identiques (1 / 0). On pourra utiliser la fonction [`findall`](https://docs.python.org/3/library/re.html#re.findall) de la librairie `re`.\n",
    "Puis l'appliquer sur l'exemple précédent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad43cf84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model scored 1.0\n"
     ]
    }
   ],
   "source": [
    "from re import findall\n",
    "\n",
    "def score(prediction, reference):\n",
    "    if reference is None:\n",
    "        return 0.0\n",
    "\n",
    "    numbers = findall(r\"\\d+\", prediction)\n",
    "    return 1.0 if numbers and numbers[-1] == reference else 0.0\n",
    "\n",
    "value = score(response, answer)\n",
    "print(f\"The model scored {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ec5088",
   "metadata": {},
   "source": [
    "Nous avons l'ensemble des éléments nécessaire pour définir la classe `GSM8KBenchmark` depuis la classe `Benchmark` que nous avons défini précédemment.\n",
    "\n",
    "**Consigne** : Définir cette classe comme sous-classe de `Benchmark`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d83f4394",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GSM8KBenchmark(Benchmark):\n",
    "    name = \"GSM8K\"\n",
    "\n",
    "    def load_data(self):\n",
    "        return load_dataset(\"gsm8k\", \"main\", split=\"test\")\n",
    "\n",
    "    def build_chain(self, model):\n",
    "        return (\n",
    "            {\"question\": RunnablePassthrough()}\n",
    "            | self.prompt\n",
    "            | model\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "    def get_reference(self, sample):\n",
    "        match = search(r\"#### (\\d+)\", sample[\"answer\"])\n",
    "        return match.group(1) if match else None\n",
    "\n",
    "    def score(self, prediction, reference):\n",
    "        if reference is None:\n",
    "            return 0.0\n",
    "        numbers = findall(r\"\\d+\", prediction)\n",
    "        return 1.0 if numbers and numbers[-1] == reference else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc3cb78",
   "metadata": {},
   "source": [
    "Il est maintenant temps de définir une fonction qui *fait* le benchmark.\n",
    "\n",
    "**Consigne** : Définir une fonction `run_benchmark` qui prend en paramètre :\n",
    "* `model_name` : le nom du modèle Ollama que l'on veut tester\n",
    "* `benchmark` : la classe benchmark que l'on souhaite tester\n",
    "* `max_samples` : le nombre maximum de questions que l'on souhaite utiliser\n",
    "\n",
    "Puisque l'object avec lequel nous travaillons est un dataset HuggingFace, pour sélectionner $n$ lignes, on utilisera \n",
    "```python\n",
    "dataset = dataset.select(range(max_samples))\n",
    "```\n",
    "De cette manière on préserve la structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2d7125af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def run_benchmark(model_name: str, benchmark: Benchmark, max_samples: int | None = None) -> dict:\n",
    "\n",
    "    model = OllamaLLM(model=model_name)\n",
    "\n",
    "    data = benchmark.load_data()\n",
    "    if max_samples:\n",
    "        data = data.select(range(max_samples))\n",
    "    chain = benchmark.build_chain(model)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for sample in tqdm(data, desc=f\"Running {benchmark.name}\"):\n",
    "        prediction = chain.invoke(sample)\n",
    "        reference = benchmark.get_reference(sample)\n",
    "        scores.append(benchmark.score(prediction, reference))\n",
    "\n",
    "    results = {\n",
    "        \"benchmark\": benchmark.name,\n",
    "        \"model\": model_name,\n",
    "        \"num_samples\": len(scores),\n",
    "        \"accuracy\": np.mean(scores),\n",
    "    }\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81de8940",
   "metadata": {},
   "source": [
    "**Consigne** : Utiliser la fonction `run_benchmark` en définissant un prompt pour GSM8K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f6bbeb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running GSM8K: 100%|██████████| 5/5 [00:50<00:00, 10.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'benchmark': 'GSM8K', 'model': 'gemma3:4b', 'num_samples': 5, 'accuracy': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_GMS8K = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=(\n",
    "        \"\"\"You are a careful mathematician. Solve the problem step by step, then display your answer in the end.\n",
    "        Question: {question}\n",
    "        Answer:\"\"\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "benchmark_GSM8K = GSM8KBenchmark(prompt=prompt_GMS8K)\n",
    "results = run_benchmark(model_name=\"gemma3:4b\", benchmark=benchmark_GSM8K, max_samples=5)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c943124",
   "metadata": {},
   "source": [
    "### HellaSwag\n",
    "\n",
    "Maintenant que nous avons réussi à le faire pour le dataset GMS8K, attaquons-nous à [HellaSwag](https://huggingface.co/datasets/Rowan/hellaswag).\n",
    "\n",
    "**Consigne** : En suivant la même approche que précédemment, implémenter une sous classe `HellaSwagBenchmark` à partir de la classe `Benchmark`. Puis utiliser la fonction `run_benchmark` pour valider votre travail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "32886901",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HellaSwagBenchmark(Benchmark):\n",
    "    name = \"HellaSwag\"\n",
    "\n",
    "    def load_data(self):\n",
    "        return load_dataset(\"hellaswag\", split=\"validation\")\n",
    "\n",
    "    def build_chain(self, model):\n",
    "        return (\n",
    "            {\n",
    "                \"context\": lambda x: x[\"ctx\"],\n",
    "                \"choices\": lambda x: \"\\n\".join(\n",
    "                    f\"{index}: {choice}\" for index, choice in enumerate(x[\"endings\"])\n",
    "                ),\n",
    "            }\n",
    "            | self.prompt\n",
    "            | model\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "    def get_reference(self, sample):\n",
    "        return str(sample[\"label\"])\n",
    "\n",
    "    def score(self, prediction, reference):\n",
    "        match = search(r\"\\d\", prediction)\n",
    "        return 1.0 if match and match.group(0) == reference else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "96a3031a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running HellaSwag: 100%|██████████| 5/5 [00:02<00:00,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'benchmark': 'HellaSwag', 'model': 'gemma3:4b', 'num_samples': 5, 'accuracy': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_HellaSwag = PromptTemplate(\n",
    "    input_variables=[\"context\", \"choices\"],\n",
    "    template=(\n",
    "        \"\"\"You will be given a context and then different choices. You need to find the most likely continuation to the context. Answer with the number of the most likely choice only.\n",
    "        Context: {context}\n",
    "        Choices: {choices}\n",
    "        Answer:\"\"\"\n",
    "    )\n",
    ")\n",
    "\n",
    "benchmark_HellaSwag = HellaSwagBenchmark(prompt=prompt_HellaSwag)\n",
    "\n",
    "results = run_benchmark(model_name=\"gemma3:4b\", benchmark=benchmark_HellaSwag, max_samples=5)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c542783c",
   "metadata": {},
   "source": [
    "## Réponses structurées\n",
    "\n",
    "Sur quelques exemples tout semble fonctionner ! Mais il y a au moins une fragilité dans notre travail : la récupération de la réponse est peu fiable et largement dépendante des prompts.\n",
    "\n",
    "\n",
    "Par exemple pour GMS8K, on aimerait avoir une réponse sous la forme d'un JSON :\n",
    "```json\n",
    "{\n",
    "  \"reasoning\": \"étapes de raisonnement\",\n",
    "  \"final_answer\": 18\n",
    "}\n",
    "```\n",
    "\n",
    "De cette manière ce serait particulièrement simple d'extraire la réponse, sans pour autant ne pas avoir de *réflexion* du modèle. En revanche pour HellaSwag, un JSON extrêment simple suffit :\n",
    "```json\n",
    "{\n",
    "    \"choice\": 2\n",
    "}\n",
    "```\n",
    "\n",
    "Pour forcer le modèle à suivre ces formats, nous allons utiliser l'option [Pydantic](https://docs.langchain.com/oss/python/langchain/structured-output). Elle s'utilise comme suit, pour GSM8K :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "988dbca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class GSM8KOutput(BaseModel):\n",
    "    reasoning: str = Field(description=\"Step-by-step reasoning\")\n",
    "    final_answer: float = Field(description=\"Final numeric answer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d855adfe",
   "metadata": {},
   "source": [
    "Concernant l'intégration dans le prompt :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25afddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"reasoning\": {\"description\": \"Step-by-step reasoning\", \"title\": \"Reasoning\", \"type\": \"string\"}, \"final_answer\": {\"description\": \"Final numeric answer\", \"title\": \"Final Answer\", \"type\": \"number\"}}, \"required\": [\"reasoning\", \"final_answer\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "parser_gsm8k = PydanticOutputParser(pydantic_object=GSM8KOutput)\n",
    "\n",
    "prompt_gsm8k = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    partial_variables={\"format_instructions\": parser_gsm8k.get_format_instructions()},\n",
    "    template=(\n",
    "        \"\"\"You are a careful mathematician. Solve the problem step by step.\n",
    "        Question: {question}\n",
    "        {format_instructions}\"\"\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(parser_gsm8k.get_format_instructions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dcc480",
   "metadata": {},
   "source": [
    "**Consigne** : Modifier la classe `Benchmark` et la sous-classe `GMS8KBenchmark` pour intégrer ces évolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "542a31d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "class Benchmark:\n",
    "    name: str\n",
    "\n",
    "    def __init__(self, prompt: PromptTemplate, parser: PydanticOutputParser):\n",
    "        self.prompt = prompt\n",
    "        self.parser = parser\n",
    "\n",
    "    def load_data(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build_chain(self, model) -> Runnable:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_reference(self, sample):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def score(self, prediction, reference):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c94f1dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GSM8KBenchmark(Benchmark):\n",
    "    name = \"GSM8K\"\n",
    "\n",
    "    def load_data(self):\n",
    "        return load_dataset(\"gsm8k\", \"main\", split=\"test\")\n",
    "\n",
    "    def build_chain(self, model):\n",
    "        return (\n",
    "            {\"question\": RunnablePassthrough()}\n",
    "            | self.prompt\n",
    "            | model\n",
    "            | self.parser\n",
    "        )\n",
    "\n",
    "    def get_reference(self, sample):\n",
    "        match = search(r\"#### (\\d+)\", sample[\"answer\"])\n",
    "        return float(match.group(1)) if match else None\n",
    "\n",
    "    def score(self, prediction: GSM8KOutput, reference: float | None):\n",
    "        if reference is None:\n",
    "            return 0.0\n",
    "        return 1.0 if prediction.final_answer == reference else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2076f24",
   "metadata": {},
   "source": [
    "**Consigne** : Utiliser la fonction `run_benchmark` et vérifier que tout fonctionne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "31e433b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running GSM8K: 100%|██████████| 5/5 [01:01<00:00, 12.25s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'benchmark': 'GSM8K', 'model': 'gemma3:4b', 'num_samples': 5, 'accuracy': 0.8}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsm8k = GSM8KBenchmark(\n",
    "    prompt=prompt_gsm8k,\n",
    "    parser=parser_gsm8k,\n",
    ")\n",
    "\n",
    "run_benchmark(\"gemma3:4b\", gsm8k, max_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ed90cd",
   "metadata": {},
   "source": [
    "**Consigne** : Réaliser la même modification pour HellaSwag, et vérifier que cela fonctionne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e678bed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HellaSwagOutput(BaseModel):\n",
    "    choice: int = Field(description=\"Index of the chosen continuation\")\n",
    "\n",
    "class HellaSwagBenchmark(Benchmark):\n",
    "    name = \"HellaSwag\"\n",
    "\n",
    "    def load_data(self):\n",
    "        return load_dataset(\"hellaswag\", split=\"validation\")\n",
    "\n",
    "    def build_chain(self, model):\n",
    "        return (\n",
    "            {\n",
    "                \"context\": lambda x: x[\"ctx\"],\n",
    "                \"choices\": lambda x: \"\\n\".join(\n",
    "                    f\"{index}: {choice}\" for index, choice in enumerate(x[\"endings\"])\n",
    "                ),\n",
    "            }\n",
    "            | self.prompt\n",
    "            | model\n",
    "            | self.parser\n",
    "        )\n",
    "\n",
    "    def get_reference(self, sample):\n",
    "        return str(sample[\"label\"])\n",
    "\n",
    "    def score(self, prediction: HellaSwagOutput, reference: str) -> float:\n",
    "        return 1.0 if str(prediction.choice) == reference else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2455f816",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running HellaSwag: 100%|██████████| 5/5 [00:15<00:00,  3.12s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'benchmark': 'HellaSwag',\n",
       " 'model': 'gemma3:4b',\n",
       " 'num_samples': 5,\n",
       " 'accuracy': 1.0}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser_hellaswag= PydanticOutputParser(pydantic_object=HellaSwagOutput)\n",
    "\n",
    "prompt_HellaSwag = PromptTemplate(\n",
    "    input_variables=[\"context\", \"choices\"],\n",
    "    partial_variables={\"format_instructions\": parser_hellaswag.get_format_instructions()},\n",
    "    template=(\n",
    "        \"\"\"You will be given a context and then different choices. You need to find the most likely continuation to the context.\n",
    "        Context: {context}\n",
    "        Choices: {choices}\n",
    "        {format_instructions}\"\"\"\n",
    "    )\n",
    ")\n",
    "\n",
    "hella_swag = HellaSwagBenchmark(\n",
    "    prompt=prompt_HellaSwag,\n",
    "    parser=parser_hellaswag,\n",
    ")\n",
    "\n",
    "run_benchmark(\"gemma3:4b\", hella_swag, max_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9acd54",
   "metadata": {},
   "source": [
    "## Pour aller plus loin\n",
    "\n",
    "On pourrait implémenter d'autres benchmark, comparer vraiment des modèles entre eux, comparer des prompts entre eux..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
