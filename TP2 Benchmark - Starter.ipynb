{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "172a7a9f",
   "metadata": {},
   "source": [
    "# TP2 - Benchmark automatique\n",
    "\n",
    "Dans ce TP nous allons définir une fonction pour mesurer les performances d'un modèle de langage via l'exécution de plusieurs benchmarks. Nous avons vu en cours trois manières de mesurer la performance d'un modèle de langage qu'on peut résumer à:\n",
    "1. **Évaluation automatique**: via un ensemble de questions dont on connait la réponse\n",
    "2. **Évaluation humaine**: qualification humaine de la réponse d'un modèle à une question\n",
    "3. **Évaluation par modèle de langage**: notation ou comparaison de réponse d'un ou plusieurs modèles par un autre modèle\n",
    "\n",
    "Nous nous intéressons ici au premier point, en particulier avec les benchmarks [GSM8K](https://huggingface.co/datasets/openai/gsm8k) et [HellaSwag](https://huggingface.co/datasets/Rowan/hellaswag).\n",
    "Dans l'ensemble du notebook nous utiliserons la librairie LangChain.\n",
    "\n",
    "Il est à garder en tête que ce notebook n'a qu'une portée pédagogique et n'est pas forcément à jour puisque le domaine évolue rapidement, ni que les pratiques sont celles validées par l'industrie.\n",
    "\n",
    "## Uniformisation des benchmarks\n",
    "\n",
    "Pour chaque benchmark que l'on considère, nous avons besoin de plusieurs informations :\n",
    "* **Dataset** : une fonction pour charger les questions du benchmark\n",
    "* **Référence** : une fonction capable d'identifier la réponse attentue\n",
    "* **Prompt** : un prompt qui permet de demander correctement au modèle de répondre à la question\n",
    "* **Chaîne** : une fonction qui renvoie la chaîne de traitement de LangChain\n",
    "* **Score** : une fonction qui score la performance d'un modèle sur une question\n",
    "\n",
    "Nous allons commencer par créer une classe qui regroupe ces desiderata :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd75374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "class Benchmark:\n",
    "    name: str\n",
    "\n",
    "    def __init__(self, prompt: PromptTemplate):\n",
    "        self.prompt = prompt\n",
    "\n",
    "    def load_data(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build_chain(self, model) -> Runnable:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_reference(self, sample):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def score(self, prediction, reference):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ab41df",
   "metadata": {},
   "source": [
    "Pour rendre cette classe plus concrète, commençons par travailler avec le benchmark [GSM8K](https://huggingface.co/datasets/openai/gsm8k).\n",
    "\n",
    "### Benchmark GSM8K\n",
    "\n",
    "On commence par charger le dataset et observer une question.\n",
    "\n",
    "**Consigne** : Résoudre la question *à la main* et vérifier votre réponse. On recommande d'explorer plusieurs questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93979ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np; np.random.seed(42)\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"gsm8k\", \"main\")\n",
    "dataset = dataset[\"test\"]\n",
    "\n",
    "print(f\"Number of questions: {len(dataset)}\")\n",
    "index = 0\n",
    "print(\"Example of question:\\n\", dataset[index][\"question\"])\n",
    "print(\"And its answer:\\n\", dataset[index][\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d797f0",
   "metadata": {},
   "source": [
    "Après avoir inspecté plusieurs éléments du dataset, on remarque que la réponse finale est placée après la chaîne de caractères \"####\".\n",
    "\n",
    "**Consigne**: Construire une fonction `get_reference` qui prend en argument un élément de GMS8K (dictionnaire avec question et réponse) et renvoie la réponse attendue (string). On pourra utiliser la fonction [`search`](https://docs.python.org/3/library/re.html#re.search) de la librairie [`re`](https://docs.python.org/3/library/re.html#).\n",
    "Puis tester cette fonction sur l'exemple précédent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b336056a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c137e6a",
   "metadata": {},
   "source": [
    "Il nous reste maintenant à définir un prompt tel que l'on puisse appeler un modèle et tester notre mécanique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b899872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=(\n",
    "        \"\"\"You are a careful mathematician. Solve the problem step by step, then display your answer in the end.\n",
    "        Question: {question}\n",
    "        Answer:\"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36433b53",
   "metadata": {},
   "source": [
    "En intégrant l'appel à un modèle via Ollama sur notre ordinateur, on peut définir avec LangChain :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0676b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "model = OllamaLLM(model=\"gemma3:4b\")\n",
    "\n",
    "chain = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "index = 0\n",
    "\n",
    "question = dataset[index][\"question\"]\n",
    "answer = get_reference(dataset[index])\n",
    "response = chain.invoke(question)\n",
    "print(f\"Model answer : {response}\")\n",
    "print(f\"The answer was : {answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dd7db7",
   "metadata": {},
   "source": [
    "Il nous faut extraire la dernière valeur numérique pour obtenir automatiquement la réponse du modèle.\n",
    "\n",
    "**Consigne** : Définir une fonction `score` qui prend en paramètre la réponse du modèle et la réponse attendue puis renvoie si les deux réponses sont identiques (1 / 0). On pourra utiliser la fonction [`findall`](https://docs.python.org/3/library/re.html#re.findall) de la librairie `re`.\n",
    "Puis l'appliquer sur l'exemple précédent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad43cf84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2ec5088",
   "metadata": {},
   "source": [
    "Nous avons l'ensemble des éléments nécessaire pour définir la classe `GSM8KBenchmark` depuis la classe `Benchmark` que nous avons défini précédemment.\n",
    "\n",
    "**Consigne** : Définir cette classe comme sous-classe de `Benchmark`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83f4394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfc3cb78",
   "metadata": {},
   "source": [
    "Il est maintenant temps de définir une fonction qui *fait* le benchmark.\n",
    "\n",
    "**Consigne** : Définir une fonction `run_benchmark` qui prend en paramètre :\n",
    "* `model_name` : le nom du modèle Ollama que l'on veut tester\n",
    "* `benchmark` : la classe benchmark que l'on souhaite tester\n",
    "* `max_samples` : le nombre maximum de questions que l'on souhaite utiliser\n",
    "\n",
    "Puisque l'object avec lequel nous travaillons est un dataset HuggingFace, pour sélectionner $n$ lignes, on utilisera \n",
    "```python\n",
    "dataset = dataset.select(range(max_samples))\n",
    "```\n",
    "De cette manière on préserve la structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7125af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81de8940",
   "metadata": {},
   "source": [
    "**Consigne** : Utiliser la fonction `run_benchmark` en définissant un prompt pour GSM8K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bbeb53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c943124",
   "metadata": {},
   "source": [
    "### HellaSwag\n",
    "\n",
    "Maintenant que nous avons réussi à le faire pour le dataset GMS8K, attaquons-nous à [HellaSwag](https://huggingface.co/datasets/Rowan/hellaswag).\n",
    "\n",
    "**Consigne** : En suivant la même approche que précédemment, implémenter une sous classe `HellaSwagBenchmark` à partir de la classe `Benchmark`. Puis utiliser la fonction `run_benchmark` pour valider votre travail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32886901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a3031a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c542783c",
   "metadata": {},
   "source": [
    "## Réponses structurées\n",
    "\n",
    "Sur quelques exemples tout semble fonctionner ! Mais il y a au moins une fragilité dans notre travail : la récupération de la réponse est peu fiable et largement dépendante des prompts.\n",
    "\n",
    "\n",
    "Par exemple pour GMS8K, on aimerait avoir une réponse sous la forme d'un JSON :\n",
    "```json\n",
    "{\n",
    "  \"reasoning\": \"étapes de raisonnement\",\n",
    "  \"final_answer\": 18\n",
    "}\n",
    "```\n",
    "\n",
    "De cette manière ce serait particulièrement simple d'extraire la réponse, sans pour autant ne pas avoir de *réflexion* du modèle. En revanche pour HellaSwag, un JSON extrêment simple suffit :\n",
    "```json\n",
    "{\n",
    "    \"choice\": 2\n",
    "}\n",
    "```\n",
    "\n",
    "Pour forcer le modèle à suivre ces formats, nous allons utiliser l'option [Pydantic](https://docs.langchain.com/oss/python/langchain/structured-output). Elle s'utilise comme suit, pour GSM8K :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988dbca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class GSM8KOutput(BaseModel):\n",
    "    reasoning: str = Field(description=\"Step-by-step reasoning\")\n",
    "    final_answer: float = Field(description=\"Final numeric answer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d855adfe",
   "metadata": {},
   "source": [
    "Concernant l'intégration dans le prompt :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25afddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "parser_gsm8k = PydanticOutputParser(pydantic_object=GSM8KOutput)\n",
    "\n",
    "prompt_gsm8k = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    partial_variables={\"format_instructions\": parser_gsm8k.get_format_instructions()},\n",
    "    template=(\n",
    "        \"\"\"You are a careful mathematician. Solve the problem step by step.\n",
    "        Question: {question}\n",
    "        {format_instructions}\"\"\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(parser_gsm8k.get_format_instructions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dcc480",
   "metadata": {},
   "source": [
    "**Consigne** : Modifier la classe `Benchmark` et la sous-classe `GMS8KBenchmark` pour intégrer ces évolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542a31d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94f1dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2076f24",
   "metadata": {},
   "source": [
    "**Consigne** : Utiliser la fonction `run_benchmark` et vérifier que tout fonctionne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e433b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7ed90cd",
   "metadata": {},
   "source": [
    "**Consigne** : Réaliser la même modification pour HellaSwag, et vérifier que cela fonctionne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e678bed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2455f816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba9acd54",
   "metadata": {},
   "source": [
    "## Pour aller plus loin\n",
    "\n",
    "On pourrait implémenter d'autres benchmark, comparer vraiment des modèles entre eux, comparer des prompts entre eux..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
